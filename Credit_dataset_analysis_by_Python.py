# -*- coding: utf-8 -*-
"""21740729_CSE5DMI_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13EUjT4lFDwGUFVugzKMTDzpUj-u0Qs6n

**I. Explore, aggregate and transform the attributes**

**1.	Data Loading**
"""

#Import Module
import pandas as pd
import numpy as np

#Load the dataset
df = pd.read_csv('Credit_dataset.csv')

"""**2.	Data Exploration**"""

#Data Dimension
print(f'Number of rows and columns: {df.shape[0]} rows and {df.shape[1]} columns')

#Display the first and the last 5 rows of the dataset
df.head()

#Display the last 5 rows of the dataset
df.tail()

#Data Structure
df.info()

#Descriptive Statistics
df.describe()

"""**3.	Data Cleaning**"""

#Check any duplicated values
num_duplicates = df.duplicated().sum()
print(f'Number of duplicated values: {num_duplicates}')

#Display missing value in dataset
missing_values = df.isna().sum()
print(f'Number of missing values in each columns: {missing_values}')

#For categorical columns & discrete values : Fill missing values with the most frequent value
categorical_cols = ['history', 'purpose', 'savings', 'employ', 'status', 'others', 'property', 'housing', 'job', 'foreign','cards', 'liable']
# Loop through each categorized column
for col in categorical_cols:
    if col in df.columns:
        # Calculate mode value
        mode_value = df[col].value_counts().index[0]
        # Identify missing values
        missing_values = pd.isna(df[col])
        # Fill missing values with the most frequent value
        df[col].where(~missing_values, mode_value, inplace=True)


#For numeric columns with continuous values: Fill missing values with the mean value
# List of numeric columns
numeric_cols = ['duration', 'amount', 'age']

# Loop through each numeric column
for col in numeric_cols:
    if col in df.columns:
        # Calculate mean value
        mean_value = round(df[col].mean(),2)
        # Identify missing values
        missing_values = pd.isna(df[col])
        # Replace missing values with the mean value
        df[col].where(~missing_values, mean_value, inplace=True)

#Check if all missing values have been handled successfully
df.isna().sum()

#Define a function to transform categorical data into numeric values
def ConvertCategoricalData(df):
    for column in df.columns:
        #Check if any columns are categorical
        if df[column].dtype == 'object':
        #If the column data type is categorical, convert it to numeric
            df[column] = df[column].astype('category').cat.codes
    return df

#Apply the ConvertCategoricalData function to the dataset
df = ConvertCategoricalData(df)

#Check datastructure to ensure categorical data was converted successfully
print(df.info())

"""**4. Create New Attributes**"""

#Credit amount per duration
df['credit_amount_per_duration'] = round(df['amount'] / df['duration'],2)

#Credit amount per card
df['credit_amount_per_card'] = round(df['amount'] / df['cards'],2)
print(df)

#Export cleaned data to CSV File
df.to_csv('Assignment1_preprocessed_data.csv', index=False)

"""**II. Classification**

**1. Data Preparation for Decision Tree Classifier**
"""

#Import Module
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import KFold
from sklearn.metrics import roc_curve, auc
from sklearn import tree
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit

# Separate the features variables from target variable.
featureVariables = df.drop('Default', axis =1) # Use axis=1 to drop column, axis = 0 to drop row

# Check the first 10 rows of feature variables
print(featureVariables.head(10))
# Display the shape of feature variables
featureVariables.shape

# Separate target variable from features variables
classLabelVariables = df['Default']

#Check the first 10 rows and the shape of target variable
print(classLabelVariables.head(10))
classLabelVariables.shape
print(df['Default'].value_counts())

#Shuffle the samples of the dataframe

# Shuffle featureVariables.
featureVariables = featureVariables.sample(frac=1, random_state=0) #fracture = 1 -> return 100% number of rows.
# Shuffle classLabelVariables.
classLabelVariables = classLabelVariables.sample(frac=1, random_state=0)

#Verify that the indices of featureVariables and classLabelVariables remain aligned after shuffling.
assert all(featureVariables.index == classLabelVariables.index)

#Split data by .iloc function
x= round(0.8*len(df)) #Assign 20% of the data for testing and 80% for training
trainFeatures, trainClassLabels = featureVariables.iloc[:x], classLabelVariables.iloc[:x] # Select rows up to index x for training
testFeatures, testClassLabels = featureVariables.iloc[x:], classLabelVariables.iloc[x:]  # Select rows from index x to the end for testing
print(trainFeatures.shape)
print(testFeatures.shape)

"""**2. Build decision tree learner and perform 10-fold crossvalidation**"""

# Build decision tree learner
treeLearner1 = DecisionTreeClassifier(random_state=0)

# Train the classifier on the training data
classifier = treeLearner1.fit(trainFeatures, trainClassLabels)

#Perform 10-fold cross validation
evalResults= cross_validate(treeLearner1, # Decision tree model
               X = featureVariables, #Independent variables
               y = classLabelVariables, #Target variable
               cv = 10, #Number of folds for cross-validation
               scoring = ['accuracy', 'roc_auc']) #Evaluation metrics

#Inspect the results of the cross-validation process
print(evalResults.keys())
print(evalResults)

# Predict labels for test data using the trained classifier
y_predict = classifier.predict(testFeatures)

# True labels for the test data
y_true = testClassLabels

# Define the accuracy calculation function
def computeAccuracy(target, predicted):
    # Calculate the number of correct predictions and divide by total predictions
    accuracy = (predicted == target).sum() / len(target)
    return accuracy

# Call the function with the true labels and predicted labels
accuracy = computeAccuracy(y_true, y_predict)

# Print the accuracy
print("Accuracy of the model =", accuracy)

"""**3. Perform evaluation results**

Confusion Matrix
"""

#Calculate & print confusion matrix
print("Confusion Matrix by Count:","\n",confusion_matrix(y_true, y_predict))
print("Confusion Matrix by Ratio:", "\n",confusion_matrix(y_true, y_predict, normalize='all'))

# Display confusion matrix by seaborn and matplot library

# Create the confusion matrix
cm = confusion_matrix(y_true, y_predict)

# Plot the confusion matrix using seaborn
plt.figure(figsize=(8, 8))
sn.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['0', '1'],
            yticklabels=['0', '1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Accuracy"""

#Average accuracy after cross validation
evalResults['test_accuracy'].mean()

"""Area Under the Receiver Operating Characteristic Curve (ROC AUC)"""

#Calculate the average ROC AUC score across cross-validation folds.
print(f"ROC AUC: {round(evalResults['test_roc_auc'].mean(),2)}")

"""Cost Matrix"""

# Define the cost matrix
cost_matrix = np.array(
    [
        [0, 10],  # 10 cost for false positives
        [1, 0],   # 1 cost for false negatives
    ]
)
# Calculate the total cost based on the confusion matrix & cost matrix
total_cost = np.sum(confusion_matrix(y_true, y_predict) * cost_matrix)

print("Cost Matrix:\n", cost_matrix)
print("Total Cost:", total_cost)

"""**III. Open Discussion**

Fine-Tuning and Evaluation of Decision Tree Classifier Model
"""

#Option 1: Build Decision Tree Learner 2 by changing some parameters
treeLearner2 = DecisionTreeClassifier(max_depth=10, #Limit depth to prevent overfitting
                                      min_samples_leaf =10,
                                      min_samples_split= 10, #Increasing min_samples_split reduces tree depth and overfitting.
                                      random_state=0)

#Other steps will be the same with the first decision tree
classifier_tree2 = treeLearner2.fit(trainFeatures, trainClassLabels)
evalResults_treeLearner2= cross_validate(treeLearner2,
               X = featureVariables, #input the whole feature dataframe
               y = classLabelVariables, #input the whole class data frame
               cv = 10, #the number of cross validation
               scoring = ['accuracy', 'roc_auc'])

#Evaluation the second decision tree
print(evalResults_treeLearner2['test_accuracy'].mean())
print(evalResults_treeLearner2['test_roc_auc'].mean())

#Option 2: Build Decision Tree Learner 3 by using GridSearchCV to find best combination of data for Decision Tree Learner

#Find the best combination of parameters for Decision Tree Learner

# Create a parameter grid based on the four key Decision Tree parameters.
parameter_grid = {'criterion':["gini","entropy"],
'max_depth': [5,6,7,8,9,10, 15],
'min_samples_leaf': [ 10, 15, 20],
'min_samples_split': [10, 20, 30]}

#Select the Decision Tree classifier model
classifier = DecisionTreeClassifier()

#Set up GridSearchCV to optimize hyperparameters with 10-fold cross-validation
grid_search = GridSearchCV(classifier, parameter_grid , cv=10)

#Train the model with different hyperparameter combinations
grid_search.fit(featureVariables, classLabelVariables)

# Print the best set of hyperparameters
print("Best hyperparameters: ", grid_search.best_params_)

#Build Decision Tree Learner 3 based on this set of parameters
treeLearner3 = DecisionTreeClassifier(criterion="entropy",
                                      max_depth=8,
                                      min_samples_leaf =15,
                                      min_samples_split= 10,
                                      random_state=0)

#Other steps will be the same with the first decision tree
classifier_tree3 = treeLearner3.fit(trainFeatures, trainClassLabels)
evalResults_treeLearner3= cross_validate(treeLearner3,
               X = featureVariables,
               y = classLabelVariables,
               cv = 10,
               scoring = ['accuracy', 'roc_auc'])

#Evaluation the third decision tree
print(evalResults_treeLearner3['test_accuracy'].mean())
print(evalResults_treeLearner3['test_roc_auc'].mean())

"""Identify Key Attribute for Assessing Customer Quality"""

#Create a plot for decision tree based on the best decision tree model
plt.figure(figsize=(30, 8))  # Increase figure size for better visibility
tree.plot_tree(
    classifier_tree3,
    feature_names=trainFeatures.columns,  #Add feature names
    class_names=["0:Good", "1:Bad"],  # Class names for the target variable
    filled=True,  # Fill nodes with colors
    fontsize=6
)
plt.title("Decision Tree Classifier 3")
plt.show()

importances = treeLearner3.feature_importances_
feature_names = trainFeatures.columns

dfFeatures = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
dfFeatures = dfFeatures.sort_values(by='Importance', ascending=False)
dfFeatures['Importance'] = dfFeatures['Importance'].round(3)
print(dfFeatures)

# Print the feature importances
plt.figure(figsize=(10, 8))
sn.barplot(data=dfFeatures, x='Importance', y='Feature', color='cornflowerblue')
plt.title('Attribute Importances from Decision Tree Model', size=16)
plt.xlabel('Importance')
plt.ylabel(None)
plt.show()

"""Analyze Overfitting in Model Performance"""

from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit
# Create the figure and axis objects for plotting
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))


learningcurve_params = {
    "X": featureVariables,  # Use the entire feature dataframe
    "y": classLabelVariables,  # Use the entire class label dataframe
    "train_sizes": np.linspace(0.1, 1.0, 5),  # Training sizes: 10% to 100%
    "cv": KFold(n_splits=10, shuffle=True, random_state=0),  # 10-fold cross-validation
    "line_kw": {"marker": "o"},  #Markers on the curve
    "std_display_style":'fill_between'# Display style for standard deviation
}

# Plot the learning curve for the optimized Decision Tree model
LearningCurveDisplay.from_estimator(treeLearner3, **learningcurve_params, ax=ax)

# Customize plot legend and title
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[:2], ["Training Score", "Cross Validation Score"])
ax.set_title("Learning Curve for Decision Tree Classifier 3")

# Show the plot
plt.show()

#Improve Overfitting by Pruning Decision Tree Classifier 3
treeLearner4 = DecisionTreeClassifier(max_depth=4,
                                      min_samples_leaf = 30,
                                      min_samples_split=30,
                                      random_state=0)

#Other steps will be the same with the first decision tree
classifier_tree4 = treeLearner4.fit(trainFeatures, trainClassLabels)
evalResults_treeLearner4= cross_validate(treeLearner4,
               X = featureVariables,
               y = classLabelVariables,
               cv = 10,
               scoring = ['accuracy', 'roc_auc'])

#Evaluation the third decision tree
print(evalResults_treeLearner4['test_accuracy'].mean())
print(evalResults_treeLearner4['test_roc_auc'].mean())

# Create the figure and axis objects for plotting
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))


learningcurve_params = {
    "X": featureVariables,  # Use the entire feature dataframe
    "y": classLabelVariables,  # Use the entire class label dataframe
    "train_sizes": np.linspace(0.1, 1.0, 5),  # Training sizes: 10% to 100%
    "cv": KFold(n_splits=10, shuffle=True, random_state=0),  # 10-fold cross-validation
    "line_kw": {"marker": "o"},  #Markers on the curve
    "std_display_style":'fill_between'# Display style for standard deviation
}

# Plot the learning curve for the optimized Decision Tree model
LearningCurveDisplay.from_estimator(treeLearner4, **learningcurve_params, ax=ax)

# Customize plot legend and title
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles[:2], ["Training Score", "Cross Validation Score"])
ax.set_title("Learning Curve for Decision Tree Classifier 4")

# Show the plot
plt.show()
