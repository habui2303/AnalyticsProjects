# -*- coding: utf-8 -*-
"""21740729_CSE5DMI2024_Assignment 2_ColabFile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19tDkSDGjFZHhc60ZRiwqBej8dqst5MLE

Perform data preprocessing to prepare the data for clustering. [1 Mark]

Please discuss is it necessary to do feature scaling/normalization?

# **PART I. DATA LOADING, PREPROCESSING AND FEATURE SCALING**

# **1. Data Preprocessing**

# **1.1. Data Loading**
"""

#Data Loading
import pandas as pd
import numpy as np
df = pd.read_csv("C:\Users\buiha\Desktop\LATROBE DOC\DATA MINING\Assignment\2\Wholesale customers data - A.csv")

"""# **1.2. Data Exploration**

**Display the First and Last 5 Rows of the Dataset**
"""

#Display the first 5 rows of the dataset
df.head()

#Display the last 5 rows of the dataset
df.tail()

"""**Data Dimension**"""

#Data Dimension
print(f'Number of rows and columns: {df.shape[0]} rows and {df.shape[1]} columns')

"""**Data Structure**"""

#Data Structure
df.info()

"""**Descriptive Statistics**"""

#Descriptive Statistics
df.describe()

"""**Histogram Visualization for All Columms**"""

import pandas as pd
import matplotlib.pyplot as plt

# Create histograms for all columns
df.hist(figsize=(15, 8), bins=30, color='skyblue', edgecolor='black')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""# **1.3. Checking Data Quality: Duplicates, Missing Values, and Outliers**

**Checking for Duplicated Values**
"""

#Check any duplicated values
num_duplicates = df.duplicated().sum()
print(f'Number of duplicated values: {num_duplicates}')

"""**Checking for Missing Values**"""

#Display missing value in dataset
missing_values = df.isna().sum()
print(f'Number of missing values in each columns: {missing_values}')

"""As the result, there is no missing and duplicated values on the dataset.

**Check Outliers in Dataset**
"""

#Check outliers in the dataset
#Select the columns to plot
columns_to_plot = df.columns

# Create the box plot
plt.figure(figsize=(10, 6))
df[columns_to_plot].boxplot()
plt.title('Box Plot of All Columns to Identify Outliers')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

"""**Remove Outliers in the Dataset**"""

#Number of row before removing outliers
row_before = df.shape[0]
#Define a function to remove outliers based on the IQR method
def remove_outliers_iqr(df, columns):
    Q1 = df[columns].quantile(0.25)  # Q1
    Q3 = df[columns].quantile(0.75)  # Q3
    IQR = Q3 - Q1  # IQR

    #Define the outlier boundaries
    lower_bound = Q1 - 3*1.5 * IQR
    upper_bound = Q3 + 3*1.5 * IQR

    #Remove outliers
    df = df[~((df[columns] < lower_bound) | (df[columns] > upper_bound)).any(axis=1)]

    return df

columns_to_clean = df.columns  # Select all columns for outlier removal

df= remove_outliers_iqr(df, columns_to_clean)

#Calculate number of row after removing outliers
rows_after = df.shape[0]
#Number of outliers removed
outliers_removed = row_before - rows_after
print(f'Number of outliers removed: {outliers_removed}')

#Check the box plot of dataset after removing outliers
plt.figure(figsize=(10, 6))
df[columns_to_clean].boxplot()
plt.title('Box Plot After Removing Outliers')
plt.xticks(rotation=45)
plt.show()

"""**One-hot encode for categorical data**"""

# One-hot encode the 'Channel' column, creating new columns prefixed with 'Channel'
df = pd.get_dummies(df, columns=['Channel'], prefix=['Channel'])

df = df.astype(int)  # Converts all columns to integers, adjust as necessary

# Display the updated DataFrame
print(df.head())

"""# **2. Feature Scaling and Normalization**

**Separate target and feature variables**
"""

#features
features = df.drop(['Region'], axis=1) # 1 for column, 0 for row
labels = df[['Region']]
features.shape

from sklearn.preprocessing import StandardScaler

#Initialize step using  StandardScaler
scaler = StandardScaler()

#fit and transform on dataset
scaler.fit(features)
scaled_features = scaler.transform(features)

labels.value_counts()

print(features)

# Print the name of the 1st and 3th features
feature_names = df.columns  # Get the original feature names from the DataFrame
print(f"1st Feature Name: {feature_names[1]}")
print(f"3th Feature Name: {feature_names[3]}")

"""# **PART II. K-MEAN AND HIERARCHICAL CLUSTERING MODEL**

# **1. K-mean Clustering**

# **1.1. Find Optimal Number of Clusters for K-mean Clustering.**

# **Elbow Method**
"""

from sklearn.cluster import KMeans
# Define K values range to test
K_values = range(2, 11) #Avoiding 1 cluster, max 10 clusters for 440 samples

# Initialize an empty list
inertia_value = []
for k in K_values:
    kmeans = KMeans(n_clusters=k, random_state= 0)
    kmeans.fit(scaled_features)  # Using scaled data
    #Append x value to a list
    inertia_value.append(kmeans.inertia_)

# Visualization to find optimal number of clusters (Elbow method)
plt.figure(dpi=125)
plt.plot(K_values, inertia_value , marker='o', color='darkblue')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia Value')
plt.show()

"""The Elbow Method did not clearly show the optimal number of clusters as the plot lacked a distinct 'elbow' or sharp drop. Therefore, I will use the silhouette score â€“ second option to identify the optimal cluster count.

# **Silhouette Score Method**
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Define range of cluster values
range_n_clusters = range(2, 11)
silhouette_avg = []

# Loop over the range of cluster numbers
for num_clusters in range_n_clusters:
    # Initialize KMeans and fit the data
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(scaled_features)

    # Calculate the silhouette score and append to the list
    score = silhouette_score(scaled_features, cluster_labels)
    silhouette_avg.append(score)
    print(f"Number of clusters = {num_clusters}, Silhouette Score = {score}")

# Plot silhouette scores for each K
plt.plot(range_n_clusters, silhouette_avg, 'bx-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis for Optimal K')
plt.show()

"""The silhouette score for the K-Means algorithm is highest when k = 2, indicating that the optimal number of clusters is 2. Therefore, we will use 2 clusters for the analysis.

# **1.2. Perform K-means Clustering.**

**Initialize and Train Algorithm on Scaled Data**
"""

from sklearn.cluster import KMeans
#Initialize the algorithm
kmeans_estimator = KMeans(n_clusters=2, random_state=0) #Using optimal number of clusters
#Train the algorithm & fit the model on scaled data
k_clusters = kmeans_estimator.fit(scaled_features)

"""**Cluster Labels, Centers, and Inertia Summary**"""

#Labels for each data points
print("Cluster labels for each data point:" ,k_clusters.labels_)
print("Cluster labels shape:",k_clusters.labels_.shape)

#Cluster Centers
print("Cluster centers:\n", k_clusters.cluster_centers_)
print("Cluster centers shape:\n", k_clusters.cluster_centers_.shape)

#Sum of quared distance
print(k_clusters.inertia_)

"""**K-Means Clustering Visualization**"""

#Visualization
pointlabels = k_clusters.labels_
clusterCenters = k_clusters.cluster_centers_

#Plotting data point
plt.figure(dpi=150)
plt.scatter(x=scaled_features[:, 1], y=scaled_features[:,3], c=pointlabels, cmap='viridis')
#Plot cluster centers
plt.scatter(x = clusterCenters[:,1],
            y = clusterCenters[:,3],
            marker = 'x',
            c = 'r')

plt.xlabel('x')
plt.ylabel('y')
plt.title('K-means Clustering with Optimal Number of Clusters')
plt.colorbar(label='Cluster Label')
plt.grid(True)
plt.show()

"""# **2. Hierarchical Clustering**

# **2.1. Find Optimal Number of Clusters for Hierarchical Clustering**
"""

from scipy.cluster.hierarchy import dendrogram
from sklearn.cluster import AgglomerativeClustering

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram
    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

# setting distance_threshold=0 ensures we compute the full tree.
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)

model = model.fit(scaled_features)
plt.title("Hierarchical Clustering Dendrogram")
# plot the top three levels of the dendrogram
plot_dendrogram(model, truncate_mode="level")
plt.xlabel("Number of points in node.")
plt.show()

"""From this dendrogram, the optimal number of clusters is two.

# **2.2. Perform Hierarchical Clustering.**

**Label Transformation for Hierarchical Clustering Analysis**
"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans

# Function to transform labels to numeric values
def transformLabels(y):
    if y == 1:
        return 2
    elif y == 2:
        return 1
    else:  # For y == 3
        return 0

# Apply the transformLabels function to the 'Region' column of the DataFrame
labels = df['Region'].apply(transformLabels)

# Display the transformed labels
labels

label_counts = labels.value_counts()
print(label_counts)

"""**Initialize Algorithm and Label Prediction**"""

#Initialize the algorithm
hier_cluster = AgglomerativeClustering(n_clusters=2,  linkage='average')
#train algorithm
hier_cluster.fit(scaled_features)

from sklearn.metrics import accuracy_score
predictedLabels = hier_cluster.labels_
print('Accuracy Score: ', accuracy_score(labels,predictedLabels))

"""**Cluster Prediction and Centroid Calculation Using NearestCentroid**"""

#Predict
print(predictedLabels)

# Use NearestCentroid to compute centroids based on predictedLabels
from sklearn.neighbors import NearestCentroid
centroid_model = NearestCentroid()
centroid_model.fit(scaled_features, predictedLabels)

# Get the centroids
centroids = centroid_model.centroids_
print(centroids)

"""**Hierarchical Clustering Visualization**"""

#Plot the actual label
plt.figure(dpi=150)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) #1 row 2 columns
ax1.scatter(x=scaled_features[:, 1],
            y=scaled_features[:, 3],
            c=labels,
            marker ='o',
            label='Original Labels')
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.legend()

#Plot the predicted label
ax2.scatter(x=scaled_features[:, 1],
            y=scaled_features[:, 3],
            c=predictedLabels,
            marker ='^',
            label='Predicted Labels')
ax2.scatter(x=centroids[:, 1],
            y=centroids[:, 3],
            marker='x',
            c='r',
            label='Centroids')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.legend()

plt.show()

"""# **PART III. COMPARE AND ANALYZE THE CLUSTERING RESULTS**

# **Evaluate Silhouette Score**
"""

from sklearn.metrics import silhouette_score

#K-means Clustering
score_KMeans = silhouette_score(scaled_features, kmeans_estimator.fit_predict(scaled_features))
print(f'Silhouette Score of K-Means Clustering: {score_KMeans:.4f}')

#Hierarchical Clustering
silhouette_hier = silhouette_score(scaled_features, hier_cluster.fit_predict(scaled_features))
print(f'Silhouette Score of Hierarchical Clustering: {silhouette_hier:.4f}')

"""# **PART IV. DISCUSSION**

# **Support Vector Machines**
"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix

#Support Vector Machine
from sklearn.svm import LinearSVC
from sklearn.svm import SVC

# Function to reverse the transformation of labels
def reverseTransformLabels(y):
    if y == 2:
        return 1
    elif y == 1:
        return 2
    else:  # For y == 0
        return 3

# Apply the reverseTransformLabels function to revert the 'Region' labels back to the original values
labels = labels.apply(reverseTransformLabels)

# Display the reversed labels
print(labels)

# Count occurrences of each label (after reversing the transformation)
label_counts = labels.value_counts()
print(label_counts)

"""Split Data to Training and Testing"""

# Split the dataset into training and testing sets (use all 7 features for training)
X_train, X_test, y_train, y_test = train_test_split(scaled_features, labels, test_size=0.3, random_state=0)

from sklearn import svm
#Initialise the algorithm
svm_model = SVC(kernel='rbf', C=10, gamma=0.1)
#Train the model using the training sets
svm_model.fit(X_train, y_train)

# Visualizing only 2 features (columns 1 & 3), set the others to their mean values.
means = X_train.mean(axis=0)
# Create a mesh grid for two features (columns 1 and 3) for visualization
x_min = X_train[:, 1].min() - 0.2
x_max = X_train[:, 1].max() + 0.2
y_min = X_train[:, 3].min() - 0.2
y_max = X_train[:, 3].max() + 0.2

#Create equally-distributed grid points for x- and y-axis
area_x_range_vals = np.arange(x_min, x_max, 0.01)
area_y_range_vals = np.arange(y_min, y_max, 0.01)
print('area_x_range_vals: ', area_x_range_vals.shape)
print('area_y_range_vals: ', area_y_range_vals.shape)

#Comebine these 2 sizes to one for visualization
xx, yy = np.meshgrid(area_x_range_vals, area_y_range_vals)
print('area_pts_x: ', xx.shape)
print('area_pts_y: ', yy.shape)

#Combine grid points
area_pts = np.c_[xx.ravel(), yy.ravel()]  #Convert to one dimension
print('area_pts: ', area_pts.shape)
area_pts

#Model predictions on the new data of grid points
area_pts_full = np.tile(means, (xx.ravel().shape[0], 1))

# Set column 1 (Fresh) to the mesh grid's x values
area_pts_full[:, 1] = xx.ravel()

# Set column 3 (Grocery) to the mesh grid's y values
area_pts_full[:, 3] = yy.ravel()

#Predict using the full feature set
area_pts_class = svm_model.predict(area_pts_full)
#Reshape to 2-dimenstion for output prediction
area_pts_class = area_pts_class.reshape(xx.shape)
print(area_pts_class.shape)

# Visualization
fig, ax = plt.subplots(dpi=150)

# Plot the decision regions using contourf
contourf = ax.contourf(xx, yy, area_pts_class, cmap='coolwarm', alpha=0.6)

# Add the decision boundaries
contour = ax.contour(xx, yy, area_pts_class, levels=np.unique(area_pts_class), colors='black', linewidths=2)

# Scatter plot of the training data points
scatter = ax.scatter(x=X_train[:, 1], y=X_train[:, 3], c=y_train, cmap='coolwarm', edgecolors='k')

# Set x and y limits
ax.set_xlim(x_min, x_max)
ax.set_ylim(y_min, y_max)

# x and y-axis labels
ax.set_xlabel('Fresh (Column 1)')
ax.set_ylabel('Grocery (Column 3)')

# Title of the plot
ax.set_title(f'RBF SVM Classification (3 Classes)', fontsize=13)

# Add color bar for predicted regions
plt.colorbar(contourf, ax=ax, label='Predicted Region')

# Show grid and plot
ax.grid(True)
plt.show()

"""# **Compare Supervised Method and Unsupervised Method**"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

#K-Means Clustering
# Predicted cluster labels for each data point
kmeans_predictions = kmeans_estimator.labels_
# K-Means classification report (using cluster labels as predicted and region labels as true labels)
print(f'K-Means Classification Report:\n {classification_report(labels, kmeans_predictions, zero_division=1)}\n ')
#Confusion matrix for K-Means clustering
confusion_matrix_kmeans = confusion_matrix(labels, kmeans_predictions)
print(f'Confusion Matrix for K-Means Clustering:\n {confusion_matrix_kmeans}\n\n ')

#SVM
#Predict the response for test set
predictions = svm_model.predict(X_test)
# SVM classification report
print(f"SVM Classification Report:\n {classification_report(y_test, predictions, zero_division=1)}\n ")
print(f'Confusion Matrix for SVM: \n  {confusion_matrix(y_test, predictions)}')